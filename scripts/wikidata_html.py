import time
from typing import Optional, Tuple, List, Dict
from urllib.parse import urljoin, unquote
import requests
from bs4 import BeautifulSoup, NavigableString

BASE = "https://en.wikipedia.org"
RANDOM = "https://en.wikipedia.org/wiki/Special:Random"
PHILOSOPHY = "https://en.wikipedia.org/wiki/Philosophy"
PHILOSOPHICAL = "https://en.wikipedia.org/wiki/Philosophical"

def _norm_title_from_href(href: str) -> str:
    """Return a normalized page title extracted from a /wiki/... href.
    Lowercase, spaces, no fragment.
    """
    if not href or not href.startswith("/wiki/"):
        return ""
    title = unquote(href[len("/wiki/"):]).replace("_", " ")
    title = title.split("#")[0]
    return title.strip().lower()

def normalize_url(u: str) -> str:
    if not u:
        return u
    base = u.split("#")[0].rstrip("/")
    return base.lower()

def title_of(url: str) -> str:
    return url.split("/wiki/")[-1].replace("_", " ")

def resolve_redirects(session: requests.Session, u: str) -> str:
    try:
        r = session.get(u, allow_redirects=True, timeout=(5, 20))
        return r.url
    except Exception:
        return u

def is_philosophy(session: requests.Session, u: str) -> bool:
    try:
        norm = normalize_url(resolve_redirects(session, u))
        targets = {normalize_url(PHILOSOPHY), normalize_url(PHILOSOPHICAL)}
        return norm in targets
    except Exception:
        return False

def in_ignored_container(tag) -> bool:
    for parent in tag.parents:
        if parent.name in ("table", "span") or parent.name == "figcaption":
            return True
        if parent.name == "sup" and "reference" in (parent.get("class") or []):
            return True
        classes = parent.get("class") or []
        joined = " ".join(classes)
        if any(k in joined for k in ("hatnote", "note", "sidebar", "mbox", "navbox", "thumb", "vertical-navbox")):
            return True
        # also skip footnotes/reference lists
        if any(k in joined for k in ("references", "reflist", "footnotes")):
            return True
        if parent.get("role") == "note":
            return True
    return False

def good_article_href(href: Optional[str]) -> bool:
    if not href: return False
    if not href.startswith("/wiki/"): return False
    bad = ("File:", "Wikipedia:", "Portal:", "Special:", "Help:",
           "Template:", "Template_talk:", "Talk:", "Category:", "Main_Page")
    return not any(href[6:].startswith(p) for p in bad)

def strip_parentheses(html_bytes: bytes) -> bytes:
    s = html_bytes.decode("utf-8", errors="ignore")
    out, paren, in_tag = [], 0, False
    for ch in s:
        if ch == "<": in_tag = True
        if not in_tag:
            if ch == "(": paren += 1
            if paren == 0: out.append(ch)
            if ch == ")" and paren > 0: paren -= 1
        else:
            out.append(ch)
        if ch == ">": in_tag = False
    return "".join(out).encode("utf-8")

def _link_is_in_parentheses(p_tag, target_a) -> bool:
    paren = 0
    for node in p_tag.descendants:
        if node is target_a:
            return paren > 0
        if isinstance(node, NavigableString):
            for ch in str(node):
                if ch == "(":
                    paren += 1
                elif ch == ")":
                    if paren > 0:
                        paren -= 1
    return False

# function generated by ChatGPT
def first_link(session: requests.Session, url: str) -> Optional[Tuple[str, str]]:
    r = session.get(url, allow_redirects=True, timeout=(5, 20))
    r.raise_for_status()
    soup = BeautifulSoup(r.content, "html.parser")
    content = soup.find("div", id="mw-content-text")
    if not content: return None
    for p in content.select("div.mw-parser-output > p"):
        for a in p.find_all("a", recursive=True):
            if _link_is_in_parentheses(p, a):
                continue
            if in_ignored_container(a):
                continue
            href = a.get("href")
            if good_article_href(href):
                # denylist certain targets like Ancient Greek
                norm_title = _norm_title_from_href(href)
                if norm_title in SKIP_TITLES:
                    continue
                para_text = p.get_text()
                link_text = a.get_text()
                link_pos = para_text.find(link_text)
                if link_pos == -1:
                    link_sentence = para_text.strip()
                else:
                    start = max(para_text.rfind('.', 0, link_pos),
                                para_text.rfind('?', 0, link_pos),
                                para_text.rfind('!', 0, link_pos))
                    end = min((para_text.find('.', link_pos),
                               para_text.find('?', link_pos),
                               para_text.find('!', link_pos)))
                    if start == -1: start = 0
                    else: start += 1
                    if end == -1: end = len(para_text)
                    link_sentence = para_text[start:end].strip()
                return urljoin(BASE, href), link_sentence
    # safer fallback: only consider anchors within paragraph tags
    for p in content.find_all("p"):
        for a in p.find_all("a", recursive=True):
            if in_ignored_container(a):
                continue
            if _link_is_in_parentheses(p, a):
                continue
            href = a.get("href")
            if good_article_href(href):
                norm_title = _norm_title_from_href(href)
                if norm_title in SKIP_TITLES:
                    continue
                para_text = p.get_text()
                link_text = a.get_text()
                link_pos = para_text.find(link_text)
                if link_pos == -1:
                    link_sentence = para_text.strip()
                else:
                    start = max(para_text.rfind('.', 0, link_pos),
                                para_text.rfind('?', 0, link_pos),
                                para_text.rfind('!', 0, link_pos))
                    end = min((para_text.find('.', link_pos),
                               para_text.find('?', link_pos),
                               para_text.find('!', link_pos)))
                    if start == -1: start = 0
                    else: start += 1
                    if end == -1: end = len(para_text)
                    link_sentence = para_text[start:end].strip()
                return urljoin(BASE, href), link_sentence
    return None

def steps_to_philosophy(session: requests.Session, run_id: int, out_dir: str,
                        start: Optional[str], max_steps: int, delay_s: float) -> Dict:
    url = start or session.get(RANDOM, allow_redirects=True, timeout=(5, 20)).url
    url = resolve_redirects(session, url)
    seen = set()
    path_urls: List[str] = [url]
    link_sentences: List[str] = [""]
    stop_reason = "max_steps_exceeded"

    for _ in range(max_steps):
        if is_philosophy(session, url):
            stop_reason = "reached_philosophy"
            break
        if url in seen:
            stop_reason = "loop"
            break
        seen.add(url)

        nxt = first_link(session, url)
        if not nxt:
            stop_reason = "dead_end"
            break

        next_url, sentence = nxt
        next_url = resolve_redirects(session, next_url)
        path_urls.append(next_url)
        link_sentences.append(sentence)
        url = next_url
        time.sleep(delay_s)

    return {
        "path_urls": path_urls,
        "stop_reason": stop_reason,
        "sentences": link_sentences,
    }